# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: onnxmodel.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='onnxmodel.proto',
  package='onnxmodel',
  syntax='proto3',
  serialized_options=b'\n\035com.mlcore.examples.onnxmodelB\016OnnxModelProtoP\001\242\002\004ONNX',
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x0fonnxmodel.proto\x12\tonnxmodel\"h\n\x10InferenceRequest\x12\x14\n\x0csepal_length\x18\x01 \x01(\x02\x12\x13\n\x0bsepal_width\x18\x02 \x01(\x02\x12\x14\n\x0cpetal_length\x18\x03 \x01(\x02\x12\x13\n\x0bpetal_width\x18\x04 \x01(\x02\"\x1f\n\nPrediction\x12\x11\n\tpred_prob\x18\x01 \x01(\x02\x32S\n\nInferencer\x12\x45\n\rMakeInference\x12\x1b.onnxmodel.InferenceRequest\x1a\x15.onnxmodel.Prediction\"\x00\x42\x38\n\x1d\x63om.mlcore.examples.onnxmodelB\x0eOnnxModelProtoP\x01\xa2\x02\x04ONNXb\x06proto3'
)




_INFERENCEREQUEST = _descriptor.Descriptor(
  name='InferenceRequest',
  full_name='onnxmodel.InferenceRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='sepal_length', full_name='onnxmodel.InferenceRequest.sepal_length', index=0,
      number=1, type=2, cpp_type=6, label=1,
      has_default_value=False, default_value=float(0),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
    _descriptor.FieldDescriptor(
      name='sepal_width', full_name='onnxmodel.InferenceRequest.sepal_width', index=1,
      number=2, type=2, cpp_type=6, label=1,
      has_default_value=False, default_value=float(0),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
    _descriptor.FieldDescriptor(
      name='petal_length', full_name='onnxmodel.InferenceRequest.petal_length', index=2,
      number=3, type=2, cpp_type=6, label=1,
      has_default_value=False, default_value=float(0),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
    _descriptor.FieldDescriptor(
      name='petal_width', full_name='onnxmodel.InferenceRequest.petal_width', index=3,
      number=4, type=2, cpp_type=6, label=1,
      has_default_value=False, default_value=float(0),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=30,
  serialized_end=134,
)


_PREDICTION = _descriptor.Descriptor(
  name='Prediction',
  full_name='onnxmodel.Prediction',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='pred_prob', full_name='onnxmodel.Prediction.pred_prob', index=0,
      number=1, type=2, cpp_type=6, label=1,
      has_default_value=False, default_value=float(0),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=136,
  serialized_end=167,
)

DESCRIPTOR.message_types_by_name['InferenceRequest'] = _INFERENCEREQUEST
DESCRIPTOR.message_types_by_name['Prediction'] = _PREDICTION
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

InferenceRequest = _reflection.GeneratedProtocolMessageType('InferenceRequest', (_message.Message,), {
  'DESCRIPTOR' : _INFERENCEREQUEST,
  '__module__' : 'onnxmodel_pb2'
  # @@protoc_insertion_point(class_scope:onnxmodel.InferenceRequest)
  })
_sym_db.RegisterMessage(InferenceRequest)

Prediction = _reflection.GeneratedProtocolMessageType('Prediction', (_message.Message,), {
  'DESCRIPTOR' : _PREDICTION,
  '__module__' : 'onnxmodel_pb2'
  # @@protoc_insertion_point(class_scope:onnxmodel.Prediction)
  })
_sym_db.RegisterMessage(Prediction)


DESCRIPTOR._options = None

_INFERENCER = _descriptor.ServiceDescriptor(
  name='Inferencer',
  full_name='onnxmodel.Inferencer',
  file=DESCRIPTOR,
  index=0,
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_start=169,
  serialized_end=252,
  methods=[
  _descriptor.MethodDescriptor(
    name='MakeInference',
    full_name='onnxmodel.Inferencer.MakeInference',
    index=0,
    containing_service=None,
    input_type=_INFERENCEREQUEST,
    output_type=_PREDICTION,
    serialized_options=None,
    create_key=_descriptor._internal_create_key,
  ),
])
_sym_db.RegisterServiceDescriptor(_INFERENCER)

DESCRIPTOR.services_by_name['Inferencer'] = _INFERENCER

# @@protoc_insertion_point(module_scope)
